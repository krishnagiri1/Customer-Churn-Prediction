{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6458d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "\n",
    "# Load MNIST dataset\n",
    "mnist_data = sio.loadmat('C:/Users/kvkgi/Downloads/Project 2/Project 2/basecode/mnist_all.mat')\n",
    "\n",
    "# Load CelebA dataset\n",
    "with open('C:/Users/kvkgi/Downloads/Project 2/Project 2/basecode/face_all.pickle', 'rb') as f:\n",
    "    celeb_data = pickle.load(f)\n",
    "\n",
    "# MNIST data contains training and testing data matrices for each digit (0 to 9)\n",
    "# CelebA data contains a data matrix and a corresponding labels vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6274f2",
   "metadata": {},
   "source": [
    "## 1.2 Splitting MNIST Training Data into Training and Validation Sets\n",
    "We'll randomly split the 60,000 MNIST training samples into 50,000 for training and 10,000 for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54651571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_mnist_data(mnist_data, validation_size=10000):\n",
    "    train_data, train_labels = [], []\n",
    "    test_data, test_labels = [], []\n",
    "\n",
    "    # Concatenate data from all classes\n",
    "    for i in range(10):\n",
    "        train_data.append(mnist_data[f'train{i}'])\n",
    "        test_data.append(mnist_data[f'test{i}'])\n",
    "        train_labels.append(np.full(mnist_data[f'train{i}'].shape[0], i))\n",
    "        test_labels.append(np.full(mnist_data[f'test{i}'].shape[0], i))\n",
    "    \n",
    "    train_data = np.vstack(train_data)\n",
    "    train_labels = np.concatenate(train_labels)\n",
    "    test_data = np.vstack(test_data)\n",
    "    test_labels = np.concatenate(test_labels)\n",
    "    \n",
    "    # Shuffle and split for validation\n",
    "    indices = np.arange(train_data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    validation_data = train_data[indices[:validation_size]]\n",
    "    validation_labels = train_labels[indices[:validation_size]]\n",
    "    train_data = train_data[indices[validation_size:]]\n",
    "    train_labels = train_labels[indices[validation_size:]]\n",
    "\n",
    "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
    "\n",
    "train_data, train_labels, val_data, val_labels, test_data, test_labels = split_mnist_data(mnist_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ea9e3",
   "metadata": {},
   "source": [
    "## 1.3 Feature Selection\n",
    "Remove features with the same value across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7859e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection(data):\n",
    "    # Remove features that have the same value across all samples\n",
    "    feature_variability = np.std(data, axis=0) > 0\n",
    "    return data[:, feature_variability], feature_variability\n",
    "\n",
    "# Apply feature selection to train and validation data\n",
    "train_data, selected_features = feature_selection(train_data)\n",
    "val_data = val_data[:, selected_features]\n",
    "test_data = test_data[:, selected_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eb9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(in_units, out_units):\n",
    "    epsilon = np.sqrt(2.0 / (in_units + out_units))  # He initialization for ReLU\n",
    "    return np.random.randn(out_units, in_units + 1) * epsilon  # +1 for bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815b157",
   "metadata": {},
   "source": [
    "## 2.2 Sigmoid Activation Function\n",
    "The sigmoid function will be used as the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c50fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # To avoid overflow, clip values of z within a range\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663b584",
   "metadata": {},
   "source": [
    "## 2.3 Neural Network Objective Function (nnObjFunction)\n",
    "This function computes the error and its gradient using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109aa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ReLU activation function and its derivative\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "\n",
    "def apply_dropout(layer_output, dropout_rate):\n",
    "    dropout_mask = (np.random.rand(*layer_output.shape) < (1 - dropout_rate)).astype(float)\n",
    "    return layer_output * dropout_mask\n",
    "\n",
    "def nn_obj_function(params, *args, is_training=True, dropout_rate=0.5):\n",
    "    n_input, n_hidden, n_class, train_data, train_labels, lambda_reg = args\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    \n",
    "    # Forward pass\n",
    "    train_data = np.hstack((train_data, np.ones((train_data.shape[0], 1))))  # Add bias to input\n",
    "    z = relu(np.dot(train_data, w1.T))  # Hidden layer activations\n",
    "    \n",
    "    # Apply dropout only during training\n",
    "    if is_training:\n",
    "        z = apply_dropout(z, dropout_rate)\n",
    "\n",
    "    z = np.hstack((z, np.ones((z.shape[0], 1))))  # Add bias to hidden layer\n",
    "    o = sigmoid(np.dot(z, w2.T))  # Output layer activations\n",
    "\n",
    "    # Clip predictions to avoid log(0)\n",
    "    epsilon = 1e-10\n",
    "    o = np.clip(o, epsilon, 1 - epsilon)\n",
    "    \n",
    "    # Calculate error with regularization\n",
    "    y = np.zeros((train_data.shape[0], n_class))\n",
    "    y[np.arange(train_data.shape[0]), train_labels] = 1\n",
    "    error = -np.sum(y * np.log(o) + (1 - y) * np.log(1 - o)) / train_data.shape[0]\n",
    "    error += (lambda_reg / (2 * train_data.shape[0])) * (np.sum(w1 ** 2) + np.sum(w2 ** 2))\n",
    "    \n",
    "    # Backpropagation\n",
    "    delta = o - y\n",
    "    grad_w2 = np.dot(delta.T, z) / train_data.shape[0] + (lambda_reg * w2) / train_data.shape[0]\n",
    "    grad_hidden = np.dot(delta, w2) * relu_derivative(z)\n",
    "    grad_hidden = grad_hidden[:, :-1]  # Remove bias gradient\n",
    "    grad_w1 = np.dot(grad_hidden.T, train_data) / train_data.shape[0] + (lambda_reg * w1) / train_data.shape[0]\n",
    "    \n",
    "    return error, np.concatenate((grad_w1.flatten(), grad_w2.flatten()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b05355f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Scaler saved successfully!\n",
      "After SMOTE: [4139 4139]\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "ğŸ” Best Parameters: {'subsample': 0.7, 'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.05, 'gamma': 0.1, 'colsample_bytree': 0.7}\n",
      "âœ… Best Cross-Validation Accuracy: 0.8166281016191191\n",
      "\n",
      "Confusion Matrix:\n",
      " [[824 211]\n",
      " [115 259]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.80      0.83      1035\n",
      "           1       0.55      0.69      0.61       374\n",
      "\n",
      "    accuracy                           0.77      1409\n",
      "   macro avg       0.71      0.74      0.72      1409\n",
      "weighted avg       0.79      0.77      0.78      1409\n",
      "\n",
      "\n",
      "ROC AUC Score: 0.74432431734222\n",
      "\n",
      "âœ… Updated model saved as churn_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 2_Preprocessing_Modeling.ipynb\n",
    "\n",
    "# ğŸ“Œ Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸ“Œ Load Dataset\n",
    "df = pd.read_csv('D:/projects/Customer-Churn-Prediction/data/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "\n",
    "# ğŸ“Œ Drop Irrelevant Column\n",
    "df.drop(['customerID'], axis=1, inplace=True)\n",
    "\n",
    "# ğŸ“Œ Handle TotalCharges: Convert to numeric (some blanks need fixing)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df.loc[:, 'TotalCharges'] = df['TotalCharges'].fillna(df['TotalCharges'].median())\n",
    "\n",
    "# ğŸ“Œ Encode Categorical Variables\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col != 'Churn':\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "\n",
    "# ğŸ“Œ Encode Target Variable\n",
    "df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# ğŸ“Œ Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "df[['tenure', 'MonthlyCharges', 'TotalCharges']] = scaler.fit_transform(df[['tenure', 'MonthlyCharges', 'TotalCharges']])\n",
    "\n",
    "# Save scaler object\n",
    "joblib.dump(scaler, 'D:/projects/Customer-Churn-Prediction/deployment/scaler.pkl')\n",
    "print(\"âœ… Scaler saved successfully!\")\n",
    "\n",
    "# ğŸ“Œ Split Dataset\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# ğŸ“Œ Handle Class Imbalance using SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", np.bincount(y_train_res))\n",
    "\n",
    "# ğŸ“Œ Hyperparameter Tuning for XGBoost\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "random_cv = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_grid,\n",
    "                               n_iter=50, cv=3, verbose=2, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "random_cv.fit(X_train_res, y_train_res)\n",
    "\n",
    "print(\"ğŸ” Best Parameters:\", random_cv.best_params_)\n",
    "print(\"âœ… Best Cross-Validation Accuracy:\", random_cv.best_score_)\n",
    "\n",
    "# ğŸ“Œ Retrain best model\n",
    "best_model = random_cv.best_estimator_\n",
    "best_model.fit(X_train_res, y_train_res)\n",
    "\n",
    "# ğŸ“Œ Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# ğŸ“Œ Evaluation\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nROC AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "# ğŸ“Œ Save the best model\n",
    "joblib.dump(best_model, 'D:/projects/Customer-Churn-Prediction/deployment/churn_model.pkl')\n",
    "print(\"\\nâœ… Updated model saved as churn_model.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08466bc6-6bbe-4e30-8e60-f3116b3e11d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
